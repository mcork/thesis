\documentclass[12pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{a4paper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps with pdflatex; use eps in DVI mode
\usepackage{amssymb}
\usepackage{epstopdf}				% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{setspace}
\usepackage{fancyvrb}
\usepackage{float}
\usepackage{datetime}


\setcounter{secnumdepth}{-1}
\doublespacing
\DefineVerbatimEnvironment{code}{Verbatim}{fontsize=\small}
\newfloat{Pseudo Code}{thp}{lof}

\begin{document}
\include{title}
\newpage
\include{originality}
\newpage
\include{abstract}
\newpage
\renewcommand{\abstractname}{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}
\begin{abstract}
I would like to firstly thank the Mavstar team for the equipment and support to help me in my thesis.\newline
Next I would like to thank my supervisors Dr Jose Guivant and Mr Michael Woods for their support and ideas to help me in completion of my Thesis.\newline
I would also like to thank Matthew P. Grosvenor for his guidance and expertise in networking.\newline
I would also like to thanks Graham Cork and Alex Lester for proof reading my thesis and correcting my grammar.\newline
Lastly I would like to thank my friends and family for their support while writing this Thesis.
\end{abstract}
\newpage
\renewcommand\contentsname{Table of Contents}
\addcontentsline{toc}{section}{Table of Contents}
\tableofcontents
\include{Nomenclature}
\newpage
\listoffigures
\addcontentsline{toc}{section}{List of Figures}
\newpage
\pagenumbering{arabic}
\setcounter{page}{9}
\section{Chapter 1: Introduction}
\subsection{1.1 - Importance of OG Sharing}
Occupancy Grids (OG) are an important part of mapping while operating mobile platforms in an ever changing urban environment. This presents many problems for the platform. One of the major problems with mapping is the need for multiple devices to map simultaneously. This allows for a faster and more accurate map to be created but creates a new set of problems. 
\newline
The major problem for multiple platforms is trying to find an efficient way to share the OG data. This is important as all platforms need to have the same map to be able to localise while also avoiding each other and plan paths around obstacles. \newline
Another major problem found in mapping is the need to setup infrastructure to connect all the platforms. This can be quite costly (either money or time) and isn\textquoteright t always possible. Adhoc networks were designed to eliminate the infrastructure requirements and allow for communication between the mobile platforms without the need of a centralised wireless access point.
\subsection{1.2 - Thesis Objectives}
The main aim of this thesis is;
\begin{itemize}
\item  to develop a protocol and framework to intelligent share occupancy grid(OG) data between processes on the platform and, 
\item create a network manager to intelligent share the OG data over an wireless adhoc network.
\end{itemize}
This involved implementing a way to share data from programs that created and manipulated the data to other programs that required it and then using the network manager to communicate it over the network. 
\subsection{1.3 - Thesis Overview}
Chapter 2 contains a look at different research into OG sharing, sharing of data over adhoc networks and different routing protocols.\newline
Chapter 3 contains a technical report of the network structure layers and the built-in advantages of each of the relevant layers.\newline
Chapter 4 is the implementation that was designed to be tested as the OG sharing protocol and framework.\newline
Chapter 5 outlines the experiments that were undertaken to ensure that the framework was an adequate solution.\newline
Chapter 6 is the results of the experiments undertaken in Chapter 5.\newline
Chapter 7 is the discussion of the results from the experiments.\newline
Chapter 8 outlines the future work that could be taken to increase efficiency of sharing OG data.\newline
Chapter 9 contains the conclusion of thesis.
\newpage
\include{LitReview}
\newpage
\section {Chapter 3: Technical Report}
Network communication can modelled on five main network layers\cite{HayNetwork}. These layers are the physical, datalink, network, transport and the application as shown in Figure~\ref{fig:NetworkStructure}. 
\begin{figure}[H]
\centering
\includegraphics[width=100mm]{Figures/Network.jpg}
\caption{Network Levels Structure \cite{HayNetwork}}
\label{fig:NetworkStructure}
\end{figure}

The transmission of data from the application layer is passed down, through the transport, network, datalink and then finally to the physical link. \newline
The physical link is the only link that has a direct connection to another devices. This base link depends on the method used to connect the devices. This could be via copper, fibre or even wireless. The physical layer sends its data using electrical pulses and waves on the electromagnetic frequency. This is received by other wireless devices for communication. \newline
The link layer sits above the physical layer. This layer is mostly implemented in the Network Interface Card (NIC). The NIC is the onboard hardware that handles the data passed  from the physical layer. The NIC uses the Message Authentication Code (MAC) protocol which gives a unique MAC address for each NIC device. It is responsible for the:
\begin{itemize}
\item Flow control, 
\item Error Detection and Correction,
\item Controlling the Duplexity.
\end{itemize}
\cite{KuroseRoss}
The link layer has a buffer that is of a limited size. This means that the NIC hardware needs to control the flow from the host to avoid causing the loss of data through data overflow. This is done through two main methods; software and hardware controlled flow. The NIC is in charge of the hardware flow control and will ensure that the data is less than the buffer. The second option is done through software. The only part of the link layer that is implemented in software is the driver which allows the program to interact with the Operating System (OS). This driver controls the the software flow and allows the user to change the setting as needed\cite{KuroseRoss}.  \newline
The error detection and correction is accomplished by using various parity methods. The parity method is given a “parity” value which can check to see if there is corruption in the packet. By using this method if a corruption has happened the program will be able to detect where the corruption is and recover from it. These approaches however will not be able to fix major corruption of the data but will be able to detect it. The NIC other main task is to establish whether the user requires a full duplex channel or only a half duplex. If the user only requires one way transmission they can control the duplexity of the channel through the driver. \newline
The two types of communication in the link layer are point to point and broadcasting. The point to point communication requires a single transmitter and a single receiver. The broadcasting communication allows for multiple senders and receivers which causes a problem with over saturation of the bandwidth from any user. The three main methods used to ensure that bandwidth is shared are:
\begin{itemize}
\item Channel Partitioning (FDM and TDM)
\item Random Access
\item Turns based sending.
\end{itemize}
\cite{KuroseRoss}
The two methods used in channel partitioning are the Frequency Division Method (FDM) and the Time Division Method (TDM). Both these methods require division of resources to allow for multiple senders and receivers. The FDM divides up the bandwidth frequency with the number of concurrent programs assigning each program a certain range of the frequency. This then allows the program to use its frequency as it sees fit. The TDM divides the time into frames and assigns each subsection to a different program. Each program is only allowed to transmit during its allocated time. \newline
The random access principle starts by transmitting on the full bandwidth. When it detects a collision it will wait a randomly determined time then retransmit all the data. This approach is simple and effective for a small quantity of programs.\newline
The last category is the turn based sending algorithms. There are many different turn based algorithms that are able to be used however the most common and effective are the  ALOHA and SLOTTED ALOHA\cite{KuroseRoss}.
The SLOTTED ALOHA method divides up the link layer frame into segments based on the bits of each frame divide by the full bandwidth on channel. The programs only transmit on the start of their period. This requires the synchronisation of all programs, to ensure that two programs don\textquoteright t transmit at the same time.
The ALOHA starts by transmitting on the full bandwidth. When it detects the collision it will determine the wait time based on a random probability. This approach doesn’t require any of the programs to be synchronised and still is a simple and effective method of collision avoidance. 
The link layer has many effective means to allow communication between devices as well as error checking/detection built in.\newline
The next layer is the network layer. This layer is implemented entirely in the software and is controlled by the OS. The network layer uses the IP protocol. Due to the dynamic nature of this layer the IP address is often issued by a Dynamic Host Control Protocol (DHCP) Server. For any fixed system, the IP address is usually statically defined. This minimises the network overhead for connection and allows for a quicker establishment of communication between the network layers  of different devices. The two main functions of this layer is either forwarding or routing. \newline
Network layer forwarding is the ability for the network layer to determine the correct input to the correct output. This layer is able to sustain many connections simultaneously. When an input is received from one connection that needs to go to another, the network layer forwarding function is responsible for ensuring that the correct input is aligned with the correct output.\newline
The second function of the network layer is the controlling of the routing between hosts. Each network layer device has a forwarding table that shows the route to all hosts it has encountered. As the network receives data from new hosts this table is automatically updated by the OS. This data is used by the forwarding function to find the correct output to the host required.\newline
The fourth layer is the transport layer. This layer requires the ports of both the destination and source hosts. This layer uses two different protocols User Datagram Protocol (UDP) and the Transmission Control Protocol (TCP). \newline
The UDP protocol is a connectionless transmission. Due to the lack of connection state, UDP overhead is small. However as the connection can’t be verified, congestion control on this level is almost impossible and this makes the protocol unreliable for data that requires a pattern.\newline
TCP is the most common connection orientated protocol. It is established using a three way handshake. This handshake ensures the reliability of the data transfer. The protocol has three main methods to ensure reliability These are 
\begin{itemize}
\item Stop’n’Wait (SW)
\item Go-Back-N (GBN)
\item Selective Repeat (SR)
\end{itemize}
\cite{KuroseRoss}
The TCP protocol can detect and handle congestion in the bandwidth. It does this by throttling the data allowed to be sent by the application layer. The disadvantage of this layer is the limited buffer. To overcome this problem the transport layer provides feedback to the application layer. This requires the application layer to ensure that the data sent is less than the buffer size of the transport layer. The TCP layer allows for a full duplex connection (i.e both sending and receiving).\newline
The SW reliability method is the slowest of the three. When it sends a packet, it waits for acknowledgment that the packet has been received.
The GBN method will send a certain number of packets. It will keep sending the packets in the window until it has received the lowest packet number\textquoteright s acknowledgment. The window will then move to the next unacknowledged packet and sending all packets greater than this one regardless whether they have been sent or acknowledged before.
The SR method also requires a window that keeps track of the the frames sent. As the sender receives an acknowledgement, it marks that packet as being received however the windows will only move to the last unacknowledged packet. The packets will only be resent when a timeout period for that packet is reached. This is the quickest method as it only resends packets that timeout.\newline
The last layer is the application layer. This layer is the program that is using the network functions. The are no common protocols for this layer.
The application layer was chosen for the intelligent sharing of the occupancy grid. This allows for all the safeguards of the lower layers to be used and allows the program to be written in higher level code with more intelligent functionality.
\begin{figure}[H]
\centering
\includegraphics[width=150mm]{Figures/RelevantNetworkStructure.jpg}
\caption{Relevant Network Levels Structure}
\label{fig:RelevantNetworkStructure}
\end{figure}
Figure~\ref{fig:RelevantNetworkStructure} shows the relevant layers for this thesis. 
\newline
Most networking is based on two main types of connection; server-client connection and node based connection. The server client method is the first main type of connection. Figure~\ref{fig:serverClient} shows the graphical representation of the approach. This type involves a centralised server that has all the information. The clients then connect to this server and request the information from it. It is usually used when their are few sources of information but many clients. This approach allows for multiple clients to get the same data easily as they know exactly where the source is. This approach fails when a platform needs to be both a client and a source.\newline
\begin{figure}[H]
\centering
\includegraphics[width=150mm]{Figures/serverClient.jpg}
\caption{Server-Client Connection}
\label{fig:serverClient}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=150mm]{Figures/nodeConnection.jpg}
\caption{Node-Node Connection}
\label{fig:node}
\end{figure}
The second main connection is a node to node based communication style as show in Figure~\ref{fig:node}. Node communication is a manager that performs as both a server and client in one program. It requires having a constant listening port that is used to receive connection requests. Once a request is received then the program establishes connection on another port for communication between the platforms. This approach allows for two way communication between the platforms. The lower network layers handle the connection to ensure that both nodes don’t send at the same time. This connection is used when both platforms need to communicate information to each other. Node to node connection also performs better than server-client when the number of sources and number of client are similar. The node connection doesn\textquoteright t always need to be both sending and receiving data but is always capable of the functionality.
\newpage
\section{Chapter 4: The Implementation}
The implementation was separated into two major logical parts to simplify the system and to allow for each section to be developed and tested individually. The modular nature of this approach allowed for both parts to be designed to increase the efficiency while keeping separate the different functionalities. Another advantage of the modular approach was it allowed the shared memory communication to be used as the major inter-process communication (IPC) for the system.
\subsection{Shared Memory Protocol}
The shared memory protocol used in this thesis was specialist design to allow separate programs to share data sets between them.  One of the main design criteria for the shared memory communication protocol (SHM Protocol) was to allow multiple sources to publish their data while ensuring that corruption of data was minimised or eliminated. The other main design criteria was the SHM Protocol had to allow multiple receivers to read the latest data and minimise the chance of that data being read being corrupted by another set of data being over-written while the original is being read. 
The SHM Protocol starts by creating a file in the platform's local RAM with a particular filename (eg \textquotedblleft OGData\textquotedblright). This \textquotedblleft file\textquotedblright  is then treated like a normal file allowing for the normal read and write functions to be used. For other people to read/write the data posted they also need to know the \textquotedblleft file\textquotedblright  name. These libraries have built in optimisations that make writing to and reading from memory more efficient than developing specific memory read and write functions for the shared memory. The structure of this \textquotedblleft file\textquotedblright  is shown in Figure~\ref{fig:shmStructure}
\begin{figure}[H]
\centering
\includegraphics[width=150mm]{Figures/sharedMemory.jpg}
\caption{Shared Memory Structure}
\label{fig:shmStructure}
\end{figure}
The file starts with the checksum. This checksum is used when the a program initially connects to the shared memory file(shm file). The program initially checks to see if the checksum is set to the magic number \textquotedblleft 230912\textquotedblright . If this checksum is set that means the file has been initialised by another program. If this value isn\textquoteright t set then the program is responsible to initialise the shared memory and set the initial variables.\newline
The next part of the file is the read and write counter. Both counters are updated when write is called.\newline
The last and biggest part of the structure is the \textquotedblleft DATA\textquotedblright  segment. This holds the actual data being communicated between the different programs.\newline
One of the major problems with multiple sources reading from, or writing to, the same data set is the increased chance of corruption. One of the main solutions to this problem is using mutexes or synchronisation locks. Mutexes and synchronisation locks are mutually exclusive locks which prevents multiple people accessing the same file at the same time. This approach is unable to be used for our purpose as we want multiple people to read the data at the same time, while only one person writes at the same time. These approaches also require hardware support to be implemented properly. Due to the above reasons the shared memory protocol needed a different approach to minimise corruption of data. \newline
\begin{figure}[H]
\centering
\includegraphics[width=150mm]{Figures/circularBuffer.jpg}
\caption{Circular Buffer for Device Drivers\cite{circularBuffer}}
\label{fig:circularBuffer}
\end{figure}
The solution chosen for the shared memory protocol is using a circular buffer to store the data. This is similar to the circular buffer used in device drivers\cite{circularBuffer} which is shown in Figure~\ref{fig:circularBuffer} however it has been adjusted to allow for multiple writers instead of only a single writer. The buffer implements an array which can hold ten different instances of the data structure being shared. The right hand side of Figure~\ref{fig:shmStructure} shows the structure of the data segment of the shared memory.
The counters included in the shared memory library (mentioned before) keep track of the next available write element of the array and the current read element. When data is written, the library updates the write counter to the next element of the array while the read counter is updated to the element that has just been written. When read is called, the program requesting data is given a pointer the the last element that was written. When data is written and the read pointer is updated, the old pointer is valid for the next nine writes and only upon the tenth write will the data be overwritten and the chance of corruption would happen. With the buffer size of ten the chance of the data being overwritten and corrupted it greatly reduced. The bigger the buffer is the smaller the chance of corruption, however the greater the buffer the more RAM is taken up in the platforms RAM. Each time read is called the update pointer will be given.\newline
The shared memory protocol is an efficient way to communicate between programs on the platform. It allows multiple sources to write to the common data set without corruption, while also allowing multiple sources to read the latest data. This protocol allows the OG to be shared to the network manager and then sent over the network to another platform\textquoteright s OG program. The protocol also ensures other programs such as path planning programs can also get the OG for its use. The shared memory protocol is also used as the major IPC communication on the platform for all programs. This is done by changing the name of the file.
\newpage
\subsection{Network}
The network manager was written on the application layer to allow for communication between the various platforms. The program was written as a node style instead of the server-client style as all nodes need to send and receive periodically. This system is superior to the server-client style as once the connection is established the data can be sent both ways across the channel without the need for a second connection. The node is based on the following pseudocode:
\newline
\begin{figure}[H]
	\centering
	\begin{code}
	    start listening on port (6000)
	    connect to host
	    loop forever{
 		    broadcast "alive" message
		    check for received data
		    if newRecvData {
		   	sync with local OG(using shared memory)
		    }
		    check to see if new data to send
		    if newData {
		  	send New Data
		    }
	    }
	\end{code}
	\caption {Pseudo Code: Network Node Implementation}
\end {figure}
The protocol was designed to have a initial listening port that all nodes try to connect to each other on. This allows all nodes to only ever need to try one port number to establish a connection. When a connection is detected, the listening port passes the connection data to randomly assigned port number. This other port accepts the connection and establishes the two way pipeline to the other node. This approach make the protocol more dynamic and can mean greater number of connections for each node.
After the connection is established the program then checks to see if new data has been sent over the adhoc network. The TCP connection allows the user to specify a few different options to determine if there is data. These methods are blocking, polling\cite{pollMan} and non-blocking\cite{recvMan}. \newline
Blocking is a method which only returns from the system call when data has been received. This is very useful if data is constantly expected and you can either guarantee the data will be there or if you only want to continue when the data is there. The main disadvantages of this approach is that it will lock up the program and stop it from completing any other tasks until it receives data. \newline
Polling is a way to check the socket connection to see if data has been stored in the buffer since it was last read. This can be a good method when you are expecting data intermittently and constantly opening and closing the connection. However the major disadvantage is that the system call will block if the connection is still open but no data is being sent over it. It is mainly used when the connection is constantly established and disconnected. \newline
For our approach the only solution that would work is using the non-blocking approach. This approach checks the connection and either returns the data being sent or returns an error code. This major disadvantage with this approach is that the node needs to do all the error checking and error-handling itself. The node program checks the return value of the \textquotedblleft receive\textquotedblright  system call and handles it appropriately. If data is found, it is then written to the shared memory for the OG implementation to deal with. If the connection has been disconnected from the remote end, then the program would disconnect the socket and wait for another connection. If the connection is still established but no data has been sent through then the nodes skips the writing to shared memory step.

\newpage
\section{Chapter 5: Experiments}
\subsection{Experiment 1 - Shared Memory}
The shared memory implementation is a way to minimise the chance of corruption of the shared data using a circular buffer. The corruption is caused by one program over writing the data currently being read. This means the program reading gets a combination of old data and new data which is the corruption. The shared memory experiments are separated into two stages for testing. \newline
\subsubsection{Experiment 1.1 - Benchmarking Shared Memory}
There is currently no benchmark for the size of data shared through the shared memory to determine at what size the corruption is reliable. The first experiment was designed to test structures of different sizes with no multiple data slots for the circular buffer (i.e circular buffer size was one). The purpose of the experiment was to show the size at which corruption first occurred and to develop a benchmark in which data corruption reliably occurred. Using the benchmark allows for future comparisons to be made.\newline
The experiment consist of two programs accessing the shared memory. The first program was responsible for writing an array of single integers into the shared memory. 
\begin{figure}[H]
\centering
\begin{code}
	set integer initial value
	loop forever{
		set all elements of array to integer
		write to share memory
	}
\end{code}
\caption {Pseudo Code: Write program for Shared Memory Testing}
\end{figure}
Two instances of this program were used to write different values to the shared memory.
\newline
The second program was reading the value from shared memory and determining if the data had been corrupted. 
\begin{figure}[H]
\centering
\begin{code}
	loop forever{
		read from shared memory
		check array for corruption
	}
\end{code}
\caption {Pseudo Code: Read program for Shared Memory Testing}
\end{figure}
Corruption was be determined by checking all the values of the array and if the integers aren’t all the same the data was corrupted. The method to obtain the benchmark is outlined below.
\newline
\underline {\bf Method}
\begin{enumerate}
\item Change Array Size
\item Run Both Write Programs and Read Program
\item Monitor Output to check if corruption
\item Repeat Steps 1-3 until corruption reliably appears
\end {enumerate}
\newpage
\subsubsection{Experiment 1.2 - Circular Buffer Size}
The second experiment relied on the benchmark obtained in the first experiment to compare the chance of corruption by varying the sizes of the circular buffer. This purpose of the experiment was to prove that the circular buffer could minimise or eliminate the corruption of data and to obtain a reliable circular buffer size for all inter-process communications.\newline
The experiment started off using an array, the size determined in experiment 1.1 as the benchmarking value. The testing procedure was similar to the method used in experiment 1.1. The circular buffer size was be increased and the tests re-run until the corruption was minimised or eliminated completely.\newline
\underline {\bf Method}
\begin{enumerate}
\item Set Array Size to benchmark size
\item Change Circular Buffer Size
\item Run Both Write Programs and Read Program
\item Monitor Output to check if corruption
\item Repeat Steps 2-4 until corruption disappears
\end {enumerate}
\newpage
\subsubsection{Experiment 1.3 - Circular Buffer Size with Multiple Writers}
The third experiment for the shared memory was to confirm the “safe” recommended value with multiple writers, under heavy computer load and to ensure that the circular buffer values were being updated correctly.\newline
The safe recommended value, for the circular buffer, was chosen to be above the value obtained in experiment 1.2. For this experiment a circular buffer of size 5 was chosen. This value should proved to be corruption free as it is higher than the obtained value. This value was then tested at high load on the computer which had multiple programs running at the same time. This allowed the experiment to show if increase CPU load and multiple programs writing to the shared memory resulted in corruption at the safe value.\newline
A status function was written to allow the read program to read the current values of the read, write counters and the checksum and allowed it to be checked on screen for accuracy.\newline
The corruption checking function iterated through the array size and checked that all values we the same.
\newline
\underline {\bf Method}
\begin{enumerate}
\item Set Array Size to benchmark size
\item Set Circular Buffer Size to "safe"  size
\item Run Read Program and 10 write programs
\item Monitor Output to check if corrupted
\item Monitor Circular Buffer Status to ensure Updating happens 
\end {enumerate}
\newpage
\subsection{Experiment 2 - Network Node Timing}
Experiment 2 was designed to test the end to end timing of the entire approach. This includes the network manager and the shared memory implementations. Timings were be able to compared with other network sharing protocols such as SCP. All platforms had their clocks synced before the testing using the built in linux NTP servers.\newline
Different sizes OG's were also shared over the network, to check to see if there was an optimal size of data being sent. The OG data was time-stamped by the initial program then written to the shared memory to be shared memory. This change was propagated through the network to the next node. The next node checked the time stamp and display the timing it took for the packet to reach it.\newline
Each data set consisted of twenty changes and the timings were averaged for that data set to minimise errors created by interference in the bandwidth. The experiment was repeated with different sizes of OG to determine the ideal size of sending data and allowing comparisons to be made against the same sizes being sent through another sharing program.\newline.
\newpage
\section{Chapter 6: Results}
\subsection{Experiment 1 - Shared Memory}
\subsubsection{Experiment 1.1 - Benchmarking Shared Memory}
\begin{figure}[H]
\centering
\includegraphics[width=150mm]{Figures/BufferSize1.jpg}
\caption {Results of Experiment 1.1 - Benchmark}
\label{fig:resultBuffer1}
\end{figure}
From Figure~\ref{fig:resultBuffer1}, It can be seen that corruption began when the the array size had 300000 elements in size. It also shows in the graph that at 500000 the data corruption was only 40\% of tests. Arrays with number of elements greater than 500000 caused segmentation faults for the program (memory out of bounds). The benchmark determined by the test thus is array size of 500000. This was then used in experiment 1.2.
\newline
\subsubsection{Experiment 1.2 - Circular Buffer Size}
\begin{figure}[H]
\centering
\includegraphics[width=150mm]{Figures/BufferSize2.jpg}
\caption {Results of Experiment 1.2 - Buffer Size = 2}
\label{fig:resultBuffer2}
\end{figure}
The result from Figure~\ref{fig:resultBuffer2} show that the implement of just a circular buffer of two reduced chance of the corruption to 10\%.
\newline
\begin{figure}[H]
\centering
\includegraphics[width=150mm]{Figures/BufferSize3.jpg}
\caption {Results of Experiment 1.2 - Buffer Size = 3}
\label{fig:resultBuffer3}
\end{figure}
The result from Figure~\ref{fig:resultBuffer3} showed that a buffer size of 3 further reduced the chance of corruption. This test showed that only 5\% of the tests showed data corruption.
\newline
\begin{figure}[H]
\centering
\includegraphics[width=150mm]{Figures/BufferSize4.jpg}
\caption {Results of Experiment 1.2 - Buffer Size = 4}
\label{fig:resultBuffer4}
\end{figure}
Figure~\ref{fig:resultBuffer4} results were obtained with a circular buffer of size 4. The graph shows that data corruption didn\textquoteright t happen for any of the test that were run.
\newpage
\subsubsection{Experiment 1.3 - Circular Buffer Size with Multiple Writers}
\begin{figure}[H]
\centering
\includegraphics[width=150mm]{Figures/BufferSize5.jpg}
\caption {Results of Experiment 1.3 - Buffer Size = 5}
\label{fig:resultBuffer5}
\end{figure}
The results of this test confirmed that for sizes greater than 4, the data still remains corruption free with more than two writing to the shared memory. 
\begin{figure}[H]
\centering
\includegraphics[width=150mm]{Figures/experiment1-3.jpg}
\caption{Results of Experiment 1.3 - Terminal Output}
\label{fig:resultTerminalOutput}
\end{figure}
Figure~\ref{fig:resultTerminalOutput} shows the output from the status function for multiple reads. It shows that the internal values were updated correctly.
\newpage
\subsection{Experiment 2 - Network Node Timing}
\begin{figure}[H]
\centering
\includegraphics[width=150mm]{Figures/Experiment2Upper.jpg}
\caption{Results of Experiment 2 - Full Graph}
\label{fig:resultExperiment2Full}
\end{figure}
Figure~\ref{fig:resultExperiment2Full} shows that the increase in size had a exponential increase in timing. This was consistent with all sizes between 5KB and 10MB.\newline
Sizes greater than 10MB, showed that the timing increase tended towards a linear trend. \newline
The final timing of 50MB was removed to show greater resolution for the other timings. The timing for 10MB was around 107 seconds.
\begin{figure}[H]
\centering
\includegraphics[width=150mm]{Figures/Experiment2Lower.jpg}
\caption{Results of Experiment 2 - Lower Values}
\label{fig:resultExperiment2Low}
\end{figure}
Figure~\ref{fig:resultExperiment2Low} is the section of Figure~\ref{fig:resultExperiment2Full} between 0 and 5KB and has produced an interesting graph. Figure~\ref{fig:resultExperiment2Low} shows that graph is a stepping function.
Comparison of SCP was unable to be done due to SCP significant overhead in creating the secure authentication overhead.
\newpage
\section{Chapter 7: Discussion}
The implementation showed the result that both the shared memory and the network manager performed the task required of them, however there were some interesting points in both which will be discussed below.
\subsection{7.1 - Shared Memory}
From Figure~\ref{fig:resultBuffer1} it can see that the corruption doesn’t happen for smaller sizes. This is due to the computer being able to read the shared memory faster than the program can overwrite it. The testing was performed on a machine of similar specs to the actual platform. Another test was performed on a computer that had greater specs than the platform and that test showed that there was no shared memory corruption. The graph also showed that only 40\% of tests were corrupted. This would be mainly caused by the reading happening immediately before another write happens. It would be possible to time the platforms exactly to eliminate corruption however that would require a custom Operating System controlling all platform timings(as well as all other OS functions). The greatest number of elements that didn’t cause a segmentation fault was 500000. This was due to an inbuilt restriction on the OS. The total memory usage increased when the circular buffer increased in size so it can be said that memory limit was not reached and thus the maximum size was set by the OS.\newline
Figure~\ref{fig:resultBuffer2} and Figure~\ref{fig:resultBuffer3} showed that the corruption was minimised as the circular buffer size was increased. This shows that the circular buffer implementation was able to eliminate some of the corruption by giving the reading programs more time before the data is overwritten. Figure 8 also shows that data corruption still exists with a circular buffer of size 3. The rate at which the circular buffer decreases wasn’t a proportional amount. This means it is impossible to calculate, in advance, the size of the buffer that would allow for corruption free sharing for any particular array size. \newline
Figure~\ref{fig:resultBuffer4} shows that corruption didn’t occur at all in the data with a circular buffer of size 4. All these values are affect by how much the computer is being used for other tasks. To allow for the difference between different computers, and the load on the computer, Experiment 1.3 was tested with a circular buffer of size 5 while having the computer perform multiple instances of the program. Figure~\ref{fig:resultBuffer5} also proves that under a heavier load (multiple programs) the corruption was still eliminated. \newline
In Experiment 1.3 Figure~\ref{fig:resultTerminalOutput} shows that the values inside the shared memory are updating based on each write. The numbers missing in the middle show that all programs are using the same set of pointers and that two people wrote between the reads. It also shows that the read counter is always one previous to the write counter. The experiment was important to ensure that the shared memory was working properly and efficiently and that all programs were using the same set of counters and not either reinitialising each time a program starts or using their own set of counters.\newline
These experimental results show that the data being shared through the shared memory is intelligently and efficiently being communicated between programs and each other or the network manager.
\newpage
\subsection{7.2 - Network Node Timing}
Experiment 2 was conducted by averaging twenty times for the same file size. This was done to help reduce errors created by interference in the wireless range. In most of the tests outliers were found and removed from the average. To establish an outlier, the average was taken and any value more than 20\% from the average was considered an outlier. The average was then recalculated without this value. This gave a more accurate value for normal timing for each file size. The graphs in the results are without outliers.\newline
Figure~\ref{fig:resultExperiment2Full} contains the values from 1KB up to 5MB. This was necessary as below 1KB was already represented on Figure ~\ref{fig:resultExperiment2Low} and only graphing up to 5MB gave a greater resolution for the other values. Above 10MB the graph tended to be more linear than exponential. \newline
However above 10 MB the values were also much less reliable. \newline
For a file size of 50MB more testing was required as more than half of the timings were over the threshold of an outlier. Most values were around 75\% above or below average with some being over 200\%. With the second set of testing the results were consistent with the first set of testing. This error was too great to include it in the graphed results.\newline
Figure~\ref{fig:resultExperiment2Full} shows the overall shape of the curve. As the file size increased the timing was increasing exponentially. This was expected as the data is increasing with a base of two and each increase will compound by a factor of two. \newline
Figure~\ref{fig:resultExperiment2Low} shows the timings between 1KB and 5KB. These timing show a very unique and complicated pattern. As can be seen from the Figure files sizes under 1KB suggested that the timing was going to plateau. This plateauing indicates a limit in the networking packet size. The shape of the first part of the graph also shows that there is a significant overhead for small packets as the difference in the timings was less the closer the file size approached to 1KB. \newline
From 1KB onwards, the graph showed a nonstandard oscillating wave around a 45 degree angle. The curve wasn\textquoteright t exactly the same for each period as the first period was larger than the other two. This was an unexpected result as the shape for all other segments were exponential. More research would need to be done to determine if the results were influenced by some interference.\newline
The network manager also could be modified to change how intelligently the data is shared. The current method only incorporated connecting and sharing of the OG data.
\newpage
\section{Chapter 8: Future Work}
Although this approach has been shown to be viable option to share the OG data there are ways that the efficiency could be increased. One of these ways is to increase the efficiency of network manager. This could be writing it on the link layer instead of the application layer. \newline
This approach would eliminate the need for all layers on the networking structure to deal with this data. The link layer would also be able to pass on the data to the next node while simultaneously passing the data up the network stack resulting in decrease of the propagation time for communication. This would also allow for simplified upper layers of the network stack as all the sharing protocol would be handle by the link layer.\newline
The main disadvantage of this approach is the protocol would need to include all the error checking and provide unique identification for each node (a possible implementation could be the MAC address currently used).\newline
Another way that was mention in the discussions was to store non-critical data until the data size was of the optimal size for sending. This will ensure that the bandwidth is used to the greatest efficiency while also ensuring data reaches the other platforms.\newline 
The networking approach could also be tested using proper network simulation programs to determine the exact efficiency of the protocol.
\newline
\newpage
\section{Chapter 9: Conclusion}
The results for the shared memory implementation showed that using a circular buffer, data corruption was eliminated using a buffer size of 5 or greater. The shared memory implementation was also shown to be an effective and intelligent protocol to communication between programs manipulating the OG and the network manager.
\newline
 The network manager proved to be an intelligent way to communicate data across the wireless adhoc network. Although it wasn\textquoteright t the most efficient program, with further research and development it could be made both more efficient and incorporate different intelligent protocols.
 \newline
 Both of these implementations when combined, provide a framework protocol for sharing OG data between platforms over a wireless adhoc network. The protocol is able to be modified to incorporate different methods for intelligent sharing data. 
\newpage
\begin{flushleft}
\addcontentsline{toc}{section}{References}
\bibliography{biblo}
\bibliographystyle{plain}
\end{flushleft}
\end{document}  